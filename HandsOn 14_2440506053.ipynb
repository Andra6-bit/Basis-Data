{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c6e8475-ff54-4a24-8f45-197d31c8aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 03:31:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created.\n",
      "Dataset Loaded.\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+--------+--------------------+------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|              Age|SibSp|Parch|          Ticket|   Fare|Embarked|           Fare_norm|Fare_outlier|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+--------+--------------------+------------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  Male|             22.0|    1|    0|       A/5 21171|   7.25|       S|0.014151057562208049|       false|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|Female|             38.0|    1|    0|        PC 17599|71.2833|       C| 0.13913573538264068|        true|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|Female|             26.0|    0|    0|STON/O2. 3101282|  7.925|       S|0.015468569817999833|       false|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|Female|             35.0|    1|    0|          113803|   53.1|       S| 0.10364429745562033|       false|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  Male|             35.0|    0|    0|          373450|   8.05|       S|0.015712553569072387|       false|\n",
      "|          6|       0|     3|    Moran, Mr. James|  Male|29.69911764705882|    0|    0|          330877| 8.4583|       Q| 0.01650950209357577|       false|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  Male|             54.0|    0|    0|           17463|51.8625|       S| 0.10122885832000206|       false|\n",
      "|          8|       0|     3|Palsson, Master. ...|  Male|              2.0|    3|    1|          349909| 21.075|       S| 0.04113566043083236|       false|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|Female|             27.0|    0|    2|          347742|11.1333|       S|0.021730754366528396|       false|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|Female|             14.0|    1|    0|          237736|30.0708|       C|0.058694292654020104|       false|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+--------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Fare_norm: double (nullable = true)\n",
      " |-- Fare_outlier: boolean (nullable = true)\n",
      "\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+--------+---------+------------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Embarked|Fare_norm|Fare_outlier|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+--------+---------+------------+\n",
      "|          0|       0|     0|   0|  0|  0|    0|    0|     0|   0|       0|        0|           0|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+--------+---------+------------+\n",
      "\n",
      "Numeric Columns: ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Fare_norm']\n",
      "Categorical Columns: ['Name', 'Sex', 'Ticket', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORT MODULE\n",
    "# ============================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, isnan, count\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# MLlib\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, RandomForestClassifier\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import (\n",
    "    MulticlassClassificationEvaluator,\n",
    "    BinaryClassificationEvaluator\n",
    ")\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. DEFINE SPARK SESSION\n",
    "# ============================================================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Titanic_MLlib\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. LOAD TITANIC DATASET\n",
    "# ============================================================\n",
    "df = spark.read.csv(\n",
    "    \"file:///home/andra/Downloads/titanic_cleaned.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(\"Dataset Loaded.\")\n",
    "df.show(10)\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. CEK MISSING VALUE\n",
    "# ============================================================\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. DETECT CATEGORICAL & NUMERIC COLUMNS\n",
    "# ============================================================\n",
    "numeric_cols = [c for c, dtype in df.dtypes if dtype in (\"double\", \"int\")]\n",
    "categorical_cols = [c for c, dtype in df.dtypes if dtype == \"string\"]\n",
    "\n",
    "print(\"Numeric Columns:\", numeric_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c1be74c-cb1d-4537-9607-219bff36999c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 03:31:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created.\n",
      "Dataset Loaded.\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+--------+--------------------+------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|              Age|SibSp|Parch|          Ticket|   Fare|Embarked|           Fare_norm|Fare_outlier|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+--------+--------------------+------------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  Male|             22.0|    1|    0|       A/5 21171|   7.25|       S|0.014151057562208049|       false|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|Female|             38.0|    1|    0|        PC 17599|71.2833|       C| 0.13913573538264068|        true|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|Female|             26.0|    0|    0|STON/O2. 3101282|  7.925|       S|0.015468569817999833|       false|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|Female|             35.0|    1|    0|          113803|   53.1|       S| 0.10364429745562033|       false|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  Male|             35.0|    0|    0|          373450|   8.05|       S|0.015712553569072387|       false|\n",
      "|          6|       0|     3|    Moran, Mr. James|  Male|29.69911764705882|    0|    0|          330877| 8.4583|       Q| 0.01650950209357577|       false|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  Male|             54.0|    0|    0|           17463|51.8625|       S| 0.10122885832000206|       false|\n",
      "|          8|       0|     3|Palsson, Master. ...|  Male|              2.0|    3|    1|          349909| 21.075|       S| 0.04113566043083236|       false|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|Female|             27.0|    0|    2|          347742|11.1333|       S|0.021730754366528396|       false|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|Female|             14.0|    1|    0|          237736|30.0708|       C|0.058694292654020104|       false|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+--------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Fare_norm: double (nullable = true)\n",
      " |-- Fare_outlier: boolean (nullable = true)\n",
      "\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+--------+---------+------------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Embarked|Fare_norm|Fare_outlier|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+--------+---------+------------+\n",
      "|          0|       0|     0|   0|  0|  0|    0|    0|     0|   0|       0|        0|           0|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+--------+---------+------------+\n",
      "\n",
      "Numeric Columns: ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Fare_norm']\n",
      "Categorical Columns: ['Name', 'Sex', 'Ticket', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORT MODULE\n",
    "# ============================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, isnan, count\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# MLlib\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, RandomForestClassifier\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import (\n",
    "    MulticlassClassificationEvaluator,\n",
    "    BinaryClassificationEvaluator\n",
    ")\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. DEFINE SPARK SESSION\n",
    "# ============================================================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Titanic_MLlib\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. LOAD TITANIC DATASET\n",
    "# ============================================================\n",
    "df = spark.read.csv(\n",
    "    \"file:///home/andra/Downloads/titanic_cleaned.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(\"Dataset Loaded.\")\n",
    "df.show(10)\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. CEK MISSING VALUE\n",
    "# ============================================================\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. DETECT CATEGORICAL & NUMERIC COLUMNS\n",
    "# ============================================================\n",
    "numeric_cols = [c for c, dtype in df.dtypes if dtype in (\"double\", \"int\")]\n",
    "categorical_cols = [c for c, dtype in df.dtypes if dtype == \"string\"]\n",
    "\n",
    "print(\"Numeric Columns:\", numeric_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a1756a1-8eb2-4701-b7be-24e417862e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 746 Test: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 03:33:02 WARN DAGScheduler: Broadcasting large task binary with size 1121.4 KiB\n",
      "25/12/04 03:33:02 WARN DAGScheduler: Broadcasting large task binary with size 1281.1 KiB\n",
      "25/12/04 03:33:03 WARN DAGScheduler: Broadcasting large task binary with size 1445.0 KiB\n",
      "25/12/04 03:33:04 WARN DAGScheduler: Broadcasting large task binary with size 1368.5 KiB\n",
      "25/12/04 03:33:11 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "25/12/04 03:33:11 WARN DAGScheduler: Broadcasting large task binary with size 1286.5 KiB\n",
      "25/12/04 03:33:12 WARN DAGScheduler: Broadcasting large task binary with size 1451.3 KiB\n",
      "25/12/04 03:33:13 WARN DAGScheduler: Broadcasting large task binary with size 1380.3 KiB\n",
      "25/12/04 03:33:20 WARN DAGScheduler: Broadcasting large task binary with size 1059.8 KiB\n",
      "25/12/04 03:33:20 WARN DAGScheduler: Broadcasting large task binary with size 1209.3 KiB\n",
      "25/12/04 03:33:20 WARN DAGScheduler: Broadcasting large task binary with size 1366.7 KiB\n",
      "25/12/04 03:33:21 WARN DAGScheduler: Broadcasting large task binary with size 1330.1 KiB\n",
      "25/12/04 03:33:24 WARN DAGScheduler: Broadcasting large task binary with size 1026.4 KiB\n",
      "25/12/04 03:33:25 WARN DAGScheduler: Broadcasting large task binary with size 1173.1 KiB\n",
      "25/12/04 03:33:26 WARN DAGScheduler: Broadcasting large task binary with size 1326.2 KiB\n",
      "25/12/04 03:33:26 WARN DAGScheduler: Broadcasting large task binary with size 1494.0 KiB\n",
      "25/12/04 03:33:28 WARN DAGScheduler: Broadcasting large task binary with size 1406.8 KiB\n",
      "25/12/04 03:33:28 WARN DAGScheduler: Broadcasting large task binary with size 1423.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+\n",
      "|Survived|label|prediction|\n",
      "+--------+-----+----------+\n",
      "|       1|  1.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       1|  1.0|       1.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       1|  1.0|       0.0|\n",
      "|       1|  1.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       1|  1.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       1|  1.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "|       0|  0.0|       0.0|\n",
      "+--------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 03:33:28 WARN DAGScheduler: Broadcasting large task binary with size 1423.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7724137931034483\n",
      "F1 Score: 0.7588230595127146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 03:33:28 WARN DAGScheduler: Broadcasting large task binary with size 1421.6 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   36|\n",
      "|  0.0|       1.0|    2|\n",
      "|  1.0|       0.0|   31|\n",
      "|  0.0|       0.0|   76|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 03:33:29 WARN DAGScheduler: Broadcasting large task binary with size 1391.2 KiB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. LABEL COLUMN (Survived)\n",
    "# ============================================================\n",
    "label_col = \"Survived\"\n",
    "\n",
    "# ============================================================\n",
    "# 2. CATEGORICAL + NUMERIC COLUMNS\n",
    "# ============================================================\n",
    "numeric_cols = ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Fare_norm']\n",
    "categorical_cols = ['Name', 'Sex', 'Ticket', 'Embarked']\n",
    "\n",
    "# ============================================================\n",
    "# 3. STRING INDEXING FOR CATEGORICAL\n",
    "# ============================================================\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + \"_index\", handleInvalid=\"keep\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 4. ONE-HOT ENCODING\n",
    "# ============================================================\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=c + \"_index\", outputCol=c + \"_vec\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 5. ASSEMBLE FINAL FEATURES\n",
    "# ============================================================\n",
    "feature_cols = numeric_cols + [c + \"_vec\" for c in categorical_cols]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6. RANDOM FOREST MODEL\n",
    "# ============================================================\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=100\n",
    ")\n",
    "\n",
    "# Label indexing\n",
    "label_indexer = StringIndexer(inputCol=label_col, outputCol=\"label\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. PIPELINE\n",
    "# ============================================================\n",
    "pipeline = Pipeline(\n",
    "    stages=indexers + encoders + [label_indexer, assembler, rf]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 8. TRAIN TEST SPLIT\n",
    "# ============================================================\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Train:\", train_df.count(), \"Test:\", test_df.count())\n",
    "\n",
    "# ============================================================\n",
    "# 9. PARAM GRID (HYPERPARAMETER TUNING)\n",
    "# ============================================================\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [100, 200])\n",
    "    .addGrid(rf.maxDepth, [5, 10])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 10. TRAIN MODEL (CROSS VALIDATION)\n",
    "# ============================================================\n",
    "cv_model = cv.fit(train_df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# ============================================================\n",
    "# 11. PREDICT\n",
    "# ============================================================\n",
    "pred = best_model.transform(test_df)\n",
    "pred.select(\"Survived\", \"label\", \"prediction\").show(20)\n",
    "\n",
    "# ============================================================\n",
    "# 12. EVALUATION\n",
    "# ============================================================\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# CONFUSION MATRIX\n",
    "pred.groupBy(\"label\", \"prediction\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32366e9c-3fcd-4b4b-8547-b43c53813f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AndraBigData)",
   "language": "python",
   "name": "andrabigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
